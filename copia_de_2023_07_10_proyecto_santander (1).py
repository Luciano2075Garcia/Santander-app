# -*- coding: utf-8 -*-
"""Copia de 2023_07_10_Proyecto Santander.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wrWPzbWWpa-LA--3DELF8YQG0d-uT4Dp

![logo Santander.jpg](data:image/jpeg;base64,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)

### Reto: People Analytics. Construcción de un modelo para segmentar un conjunto de líderes que defina las características culturales de nuestra organización e identificar el conjunto de características que deben tener los líderes que nos permitan tener un mayor desempeño alineados con la voluntad de ser un una empresa más diversa, inclusiva y heterogénea.

#### Conjunto de datos: El conjunto de datos será un conjunto de personas junto con sus características demográficas, desempeño, formación y carrera en la compañía, con un histórico de su evolución en 3 años. Entre los campos a incluirán estarán. texto en negrita




# 1.   INTRO: Proceso Análisis Exploratorio

Introducción al EDA (Exploratory Data Analysis).
Notebook por: EQUIPO SANTANDER

# 01 Import
Importamos todas las librerías necesarias para este análisis
"""

#**********************************************************************
# Import packages
#**********************************************************************
import sys
import numpy as np
import os.path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(font_scale=1.5)
sns.set_style("whitegrid")

"""Cargamos el archivo desde nuestra carpeta local

"""

# Subimos el archivo
from google.colab import files
uploaded = files.upload()

# haciendo una asignacion, en el dataframe se actua directamente creando la nueva columna , quedando modificado el DF
df_2020 = pd.read_excel ("Proyect_SPA_2020.xlsx")
df_2020.head(5)

df_2020.describe()

df_2020.info()

df_2021 = pd.read_excel ("Proyect_SPA_2021.xlsx")
df_2021.head(5)

df_2021.describe()

df_2021.info()

df_2022 = pd.read_excel ("Proyect_SPA_2022.xlsx")
df_2022.head(5)

df_2022.describe()

df_2022.info()

"""## Juntar Dataframes
Tenemos tres dataframes.

Los de los años 2020 y 2021 comparten completamente los encabezados de las columnas. Por ello, juntaremos ambos en un dataframe intermedio mediante la función "concat".


"""

df_intermed = pd.concat([df_2020, df_2021])

df_intermed.head()

df_intermed.info ()

"""Vamos a hacer data enginering para transformar los datos de evaluación de 2020 y 2021 a los mismos conceptos que 2022

Vamos a crear una columna nueva en el dataset, darle el nombre que queremos y que esa columna sea la media de las otras dos.
En otros casos solo hay que cambiar el nombre de las columnas
"""

df_20_21=df_intermed.rename( columns={'Support people - Global':'Think Customer – Pienso en el cliente - Global', 'Support people - Autovaloracion': 'Think Customer – Pienso en el cliente - Autovaloracion', 'Support people - Manager': 'Think Customer – Pienso en el cliente - Manager', 'Support people - Colaboradores': 'Think Customer – Pienso en el cliente - Colaboradores',
                                      'Embrace change - Global': 'Embrace Change – Impulso el cambio - Global', 'Embrace change - Autovaloracion': 'Embrace Change – Impulso el cambio - Autovaloracion', 'Embrace change - Manager': 'Embrace Change – Impulso el cambio - Manager', 'Embrace change - Colaboradores': 'Embrace Change – Impulso el cambio - Colaboradores'})

df_20_21.info()

df_20_21

"""Creamos las nuevas columnas unificando criterios al 2022.
Eliminamos las columnas antiguas de las que hemos calculado la media
Ordenamos las columnas

"""

df_20_21['Act Now – Actúo con rapidez - Global']= df_20_21[["Keep promises - Global", "Bring passion - Global"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Global']= df_20_21[["Actively collaborate - Global", "Show respect - Global"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Global']= df_20_21[["Truly listen - Global", "Talk straight - Global"]].mean(axis=1)
df_20_21['Act Now – Actúo con rapidez - Autovaloracion']= df_20_21[["Keep promises - Autovaloracion", "Bring passion - Autovaloracion"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Autovaloracion']= df_20_21[["Actively collaborate - Autovaloracion", "Show respect - Autovaloracion"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Autovaloracion']= df_20_21[["Truly listen - Autovaloracion", "Talk straight - Autovaloracion"]].mean(axis=1)
df_20_21['Act Now – Actúo con rapidez - Manager']= df_20_21[["Keep promises - Manager", "Bring passion - Manager"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Manager']= df_20_21[["Actively collaborate - Manager", "Show respect - Manager"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Manager']= df_20_21[["Truly listen - Manager", "Talk straight - Manager"]].mean(axis=1)
df_20_21['Act Now – Actúo con rapidez - Colaboradores']= df_20_21[["Keep promises - Colaboradores", "Bring passion - Colaboradores"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Colaboradores']= df_20_21[["Actively collaborate - Colaboradores", "Show respect - Colaboradores"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Colaboradores']= df_20_21[["Truly listen - Colaboradores", "Talk straight - Colaboradores"]].mean(axis=1)

df_20_21

df_20_21.columns

df_20_21=df_20_21.drop(columns=['Show respect - Global',
       'Truly listen - Global', 'Talk straight - Global',
       'Keep promises - Global', 'Actively collaborate - Global',
       'Bring passion - Global',
       'Show respect - Autovaloracion',
       'Truly listen - Autovaloracion', 'Talk straight - Autovaloracion',
       'Keep promises - Autovaloracion',
       'Actively collaborate - Autovaloracion',
       'Bring passion - Autovaloracion',
        'Show respect - Manager',
       'Truly listen - Manager', 'Talk straight - Manager',
       'Keep promises - Manager', 'Actively collaborate - Manager',
       'Bring passion - Manager',
        'Show respect - Colaboradores',
       'Truly listen - Colaboradores', 'Talk straight - Colaboradores',
       'Keep promises - Colaboradores', 'Actively collaborate - Colaboradores',
       'Bring passion - Colaboradores'
       ])

df_20_21.info()

df_2022.columns

df_2022.describe

orden_columnas=['ID', 'Gender', 'Country of Origin', 'Age', 'Tenure', 'Job Family',
       'Position Level 1', 'Position Level 2', 'Position Level 3',
       'Position Level 4', 'Position Level 5', 'Position Level 6',
       'Position Level 7', 'Position Level 8', 'Management Level',
       'Corporate Segment', 'Negocio/ No Negocio', 'mar_status', 'n_children',
       'field_study', 'spain_of_control', 'pct_women', 'pct_below40',
       'pct_above60', 'avg_age_sub', 'avg_ten_sub', 'pct_corp_seg', 'pct_STEM',
       'pct_mngt_lvl', 'what_performance_rating_h', 'what_performance_label_h',
       'what_performance_rating_f', 'what_performance_label_f',
       'how_performance_rating', 'how_performance_label',
       'risk_performance_rating', 'risk_performance_label',
       'overall_manager_rating', 'overall_manager_label',
       'emp_what_perf_rating', 'emp_what_perf_label', 'emp_how_perf_rating',
       'emp_how_perf_label', 'emp_rsk_perf_rating', 'emp_rsk_perf_label',
       'overall_employee_rating', 'overall_employee_label', 'year_performance',
       'Valoracion 360 Global',
       'Think Customer – Pienso en el cliente - Global',
       'Embrace Change – Impulso el cambio - Global',
       'Act Now – Actúo con rapidez - Global',
       'Move Together – Trabajo en equipo - Global',
       'Speak Up – Hablo abiertamente - Global',
       'Valoracion 360 Autovaloracion',
       'Think Customer – Pienso en el cliente - Autovaloracion',
       'Embrace Change – Impulso el cambio - Autovaloracion',
       'Act Now – Actúo con rapidez - Autovaloracion',
       'Move Together – Trabajo en equipo - Autovaloracion',
       'Speak Up – Hablo abiertamente - Autovaloracion',
       'Valoracion 360 Manager',
       'Think Customer – Pienso en el cliente - Manager',
       'Embrace Change – Impulso el cambio - Manager',
       'Act Now – Actúo con rapidez - Manager',
       'Move Together – Trabajo en equipo - Manager',
       'Speak Up – Hablo abiertamente - Manager',
       'Valoracion 360 Colaboradores',
       'Think Customer – Pienso en el cliente - Colaboradores',
       'Embrace Change – Impulso el cambio - Colaboradores',
       'Act Now – Actúo con rapidez - Colaboradores',
       'Move Together – Trabajo en equipo - Colaboradores',
       'Speak Up – Hablo abiertamente - Colaboradores']

"""# Nueva sección"""

df_20_21=df_20_21[orden_columnas]

df_20_21.head(150)

df_completo = pd.concat([df_20_21, df_2022])

# Reiniciar el índice con una nueva secuencia numérica
df_completo.reset_index(drop=True, inplace=True)

df_completo.info()

df_completo.shape

df_completo.describe()

#saco % de la columna Job Family (Javier)
job_family_counts = df_completo['Job Family'].value_counts()
job_family_percentages = (job_family_counts / len(df_completo)) * 100
print(job_family_percentages)

# Obtener los valores únicos de la columna 'year_performance' (Javier)
year_performance_values = df_completo['year_performance'].unique()
print(year_performance_values)

# Contar el número de filas por cada valor de 'year_performance' (Javier)
count_by_year_performance = df_completo['year_performance'].value_counts()

print(count_by_year_performance)

# Contar los valores nulos en 'Valoracion 360 Global' y agrupar por 'year_performance' (Javier)
null_counts_by_year = df_completo['Valoracion 360 Global'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Contar los valores nulos en 'Valoracion 360 Manager' y agrupar por 'year_performance' (Javier)
null_counts_by_year = df_completo['Valoracion 360 Manager'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Contar los valores nulos en 'Negocio / No Negocio' y agrupar por 'year_performance' (Javier) LOS PENDIENTES LOS COMPLETAMOS NOSOTROS
null_counts_by_year = df_completo['Negocio/ No Negocio'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Calcular el conteo y porcentaje de valores en 'Negocio/ No Negocio' agrupados por 'year_performance'
grouped_counts = df_completo.groupby('year_performance')['Negocio/ No Negocio'].value_counts()
grouped_percentages = grouped_counts.groupby(level=0).apply(lambda x: (x / x.sum()) * 100)

print(grouped_counts)
print(grouped_percentages)

# Contar los valores nulos en 'field_study' y agrupar por 'year_performance' (Javier)
null_counts_by_year = df_completo['field_study'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Contar filas con todos los campos completos (Javier)
complete_rows_count = df_completo.dropna().shape[0]

print("Número de filas con todos los campos completos:", complete_rows_count)

# Contar el número de campos incompletos por fila (Javier)
incomplete_fields_count = df_completo.isnull().sum(axis=1)

# Agrupar las filas por el número de campos incompletos y contar cuántas filas en cada grupo
grouped_counts = incomplete_fields_count.groupby(pd.cut(incomplete_fields_count, bins=[0, 5, 10, 15, 20, float('inf')], right=False)).count()

print(grouped_counts)

# Contar el número de campos incompletos por fila (Javier)
incomplete_fields_count = df_completo.isnull().sum(axis=1)

# Crear una nueva columna 'Incomplete Fields Count'
df_completo['Incomplete Fields Count'] = incomplete_fields_count

# Agrupar por 'year_performance' y 'Incomplete Fields Count', y contar cuántas filas en cada grupo
grouped_counts = df_completo.groupby(['year_performance', pd.cut(df_completo['Incomplete Fields Count'], bins=[0, 5, 10, 15, 20, float('inf')], right=False)])['Incomplete Fields Count'].count()

# Imprimir los resultados
print(grouped_counts)

df_completo.shape

# Filtrar el dataframe y contar campos vacíos por columna por el valor "2022" en la columna "year_performance" (Javier)
filtered_df = df_completo[df_completo['year_performance'] == 2022]

empty_fields_count = filtered_df.isnull().sum()

for column, count in empty_fields_count.items():
    print(f"Columna: {column}, Campos vacíos: {count}")

# Añadir una columna al df que sume cuantas "Position Level" tiene cada ID, la llamamos num_position_levels (Javier)
position_level_columns = ['Position Level 1', 'Position Level 2', 'Position Level 3',
                          'Position Level 4', 'Position Level 5', 'Position Level 6',
                          'Position Level 7', 'Position Level 8']


df_completo['num_position_levels'] = df_completo[position_level_columns].notnull().sum(axis=1)

print(df_completo['num_position_levels'])

df_completo.shape

df_completo

"""# NUEVO DATA FRAME PARA RELLENAR CAMPOS NULOS"""

## CREAR UN NUEVO DF PARA RELLENAR CAMPOS INCOMPLETOS
    # Crear una copia del DataFrame original y lo llamamos df_compelto_01 (Javier)
df_completo_01 = df_completo.copy()

  # Llenar con ceros los campos vacíos en la columna 'n_children' (visto con tutor)
df_completo_01['n_children'] = df_completo_01['n_children'].fillna(0)

df_completo_01

# Usando str.replace() para eliminar la cadena de texto (NEW) de los campos en la columna 'Job Family'
df_completo_01['Job Family'] = df_completo_01['Job Family'].str.replace('(NEW)', '')
unique_job_families = df_completo_01['Job Family'].unique()
print("Campos únicos en la columna 'Job Family':")
print(unique_job_families)

# Eliminar la cadena de texto "()" de todos los campos de la columna 'Job Family'
df_completo_01['Job Family'] = df_completo_01['Job Family'].str.replace(r'\(\)', '')

# Obtener el nuevo recuento de campos únicos en la columna 'Job Family'
unique_job_family_after_remove = df_completo_01['Job Family'].nunique()

# Mostrar el resultado
print("Cantidad de campos únicos en 'Job Family' después de eliminar '()':", unique_job_family_after_remove)

# Contar cuántos campos de la columna 'Job Family' incluyen el texto "()"
num_job_family_with_empty = df_completo_01['Job Family'].str.contains(r'\(\)').sum()

# Mostrar el resultado
print("Cantidad de campos en 'Job Family' que incluyen '()':", num_job_family_with_empty)

#quizás así lo hayamos solucionado (ya solo hay 175 tipos de Job Family), pero me quedaría más tranquilo si alguien con mejor crriterio en Python lo revisara

#siguiendo las instrucciones del tutor, vamos a rellenar las columnas Overall_performance_rating con el valor 2 y Overall_performance_label con Achieved
#el sentido de esto es que es el KPI más importante, es la variable objetivo a utilizar y se asume que los nulos son los valores mencionados
# Contar cuántos campos vacíos en 'overall_manager_rating'
num_empty_overall_manager_rating = df_completo_01['overall_manager_rating'].isna().sum()

# Contar cuántos campos vacíos en 'overall_manager_label'
num_empty_overall_manager_label = df_completo_01['overall_manager_label'].isna().sum()

# Mostrar los resultados
print("Cantidad de campos vacíos en 'overall_manager_rating':", num_empty_overall_manager_rating)
print("Cantidad de campos vacíos en 'overall_manager_label':", num_empty_overall_manager_label)

#vemos que hay un problema y que puede haber labels sin rating y viceversa, los buscamos
# Contar cuántas filas con valor en 'overall_manager_rating', pero sin valor en 'overall_manager_label'
num_rows_only_overall_manager_rating = df_completo_01[df_completo_01['overall_manager_rating'].notnull() & df_completo_01['overall_manager_label'].isnull()].shape[0]

# Contar cuántas filas con valor en 'overall_manager_label', pero sin valor en 'overall_manager_rating'
num_rows_only_overall_manager_label = df_completo_01[df_completo_01['overall_manager_label'].notnull() & df_completo_01['overall_manager_rating'].isnull()].shape[0]

# Mostrar los resultados
print("Cantidad de filas con valor en 'overall_manager_rating', pero sin valor en 'overall_manager_label':", num_rows_only_overall_manager_rating)
print("Cantidad de filas con valor en 'overall_manager_label', pero sin valor en 'overall_manager_rating':", num_rows_only_overall_manager_label)

#Vamos a buscar esos 41 IDs que tienen valores en label pero no en rating y vemos si podemos asignar esos ratings
# Filtrar el DataFrame para obtener las filas con valor en 'overall_manager_label', pero sin valor en 'overall_manager_rating'
filtered_df = df_completo_01[df_completo_01['overall_manager_label'].notnull() & df_completo_01['overall_manager_rating'].isnull()]

# Obtener los valores de los campos en 'overall_manager_label' que cumplen con la condición
values_only_overall_manager_label = filtered_df['overall_manager_label']

# Mostrar los resultados
print("Valores en 'overall_manager_label' que no tienen valor en 'overall_manager_rating':")
print(values_only_overall_manager_label)

#resulta que son labels con valor 0, que no es un valor en el rango de los labels que deberían de aparecer. Aquí creo que hay que preguntar al tutor si los tomamos como un error o como un Not Achieved
# una vez nos responda, podremos terminar de asignar todos los ratings y labels a los campos vacíos

#vamos a explorar cuantos ID se repiten cada año
#primero vamos a ver cuantos hay por año
unique_id_by_year_performance = df_completo_01.groupby('year_performance')['ID'].nunique()

print("Valores únicos de 'ID' filtrados por 'year_performance':")
print(unique_id_by_year_performance)

#Vamos a empezar con los IDs del 2020 que se repiten en 2021 y en 2022
# Filtrar los valores de 'ID' correspondientes al año 2020
ids_2020 = df_completo_01[df_completo_01['year_performance'] == 2020]['ID']

# Filtrar los valores de 'ID' correspondientes al año 2021
ids_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]['ID']

# Filtrar los valores de 'ID' correspondientes al año 2022
ids_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]['ID']

# Contar cuántos valores de 'ID' de 2020 se repiten en 2021
num_ids_repeated_2021 = ids_2020.isin(ids_2021).sum()

# Contar cuántos valores de 'ID' de 2020 se repiten en 2022
num_ids_repeated_2022 = ids_2020.isin(ids_2022).sum()

# Mostrar los resultados
print("Cantidad de valores de 'ID' de 2020 que se repiten en 2021:", num_ids_repeated_2021)
print("Cantidad de valores de 'ID' de 2020 que se repiten en 2022:", num_ids_repeated_2022)

#Ahora con los de 2021 que se repiten en 2022
# Filtrar los valores de 'ID' correspondientes al año 2021
ids_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]['ID']

# Filtrar los valores de 'ID' correspondientes al año 2022
ids_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]['ID']

# Contar cuántos valores de 'ID' de 2021 se repiten en 2022
num_ids_repeated_2021_to_2022 = ids_2021.isin(ids_2022).sum()

# Mostrar el resultado
print("Cantidad de valores de 'ID' de 2021 que se repiten en 2022:", num_ids_repeated_2021_to_2022)

#se repiten la gran mayoría de los valores a lo largo de los tres años (del 20 al 21 solo hay 2 que no se repiten y del 20 al 22 son 17. Del 21 al 22)
#Ahora vamos a ver si los valores que se repiten, tiene cambios en su trayectoria (tenure y Job Family)
# Filtrar los datos correspondientes al año 2020 y 2021
data_2020 = df_completo_01[df_completo_01['year_performance'] == 2020]
data_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]

# Encontrar los 'ID' que se repiten del 2020 al 2021
ids_repeated_2020_to_2021 = data_2020[data_2020['ID'].isin(data_2021['ID'])]['ID'].unique()

# Verificar si hay cambios en el valor de 'tenure' para los 'ID' que se repiten
changes_in_tenure = False
for id_ in ids_repeated_2020_to_2021:
    tenure_2020 = data_2020[data_2020['ID'] == id_]['Tenure'].iloc[0]
    tenure_2021 = data_2021[data_2021['ID'] == id_]['Tenure'].iloc[0]

    if tenure_2020 != tenure_2021:
        changes_in_tenure = True
        break

# Mostrar el resultado
if changes_in_tenure:
    print("Hay cambios en el valor de 'tenure' para los 'ID' que se repiten del 2020 al 2021.")
else:
    print("No hay cambios en el valor de 'tenure' para los 'ID' que se repiten del 2020 al 2021.")

#Hacemos lo mismo para ver los que repiten de 2021 a 2022
# Filtrar los datos correspondientes al año 2021 y 2022
data_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]
data_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]

# Encontrar los 'ID' que se repiten del 2021 al 2022
ids_repeated_2021_to_2022 = data_2021[data_2021['ID'].isin(data_2022['ID'])]['ID'].unique()

# Verificar si hay cambios en el valor de 'Tenure' para los 'ID' que se repiten
changes_in_tenure = False
for id_ in ids_repeated_2021_to_2022:
    tenure_2021 = data_2021[data_2021['ID'] == id_]['Tenure'].iloc[0]
    tenure_2022 = data_2022[data_2022['ID'] == id_]['Tenure'].iloc[0]

    if tenure_2021 != tenure_2022:
        changes_in_tenure = True
        break

# Mostrar el resultado
if changes_in_tenure:
    print("Hay cambios en el valor de 'Tenure' para los 'ID' que se repiten del 2021 al 2022.")
else:
    print("No hay cambios en el valor de 'Tenure' para los 'ID' que se repiten del 2021 al 2022.")

#Las fechas de 'tenure' son las mismas, ahora vamos a ver si algún ID cambió de Job Family durante los 3 años
# Encontrar los 'ID' que se repiten en al menos dos años
repeated_ids = df_completo_01['ID'].duplicated(keep=False)

# Filtrar los datos para incluir solo los 'ID' que se repiten en dos o más años
data_repeated_ids = df_completo_01[repeated_ids]

# Contar cuántos 'ID' han tenido cambios en 'Job Family'
count_changes_in_job_family = 0

# Iterar sobre cada 'ID' repetido
for id_ in data_repeated_ids['ID'].unique():
    data_id = data_repeated_ids[data_repeated_ids['ID'] == id_]
    job_family_values = data_id['Job Family'].unique()

    # Si hay más de un valor único de 'Job Family' para el 'ID', entonces hubo un cambio
    if len(job_family_values) > 1:
        count_changes_in_job_family += 1

# Mostrar el conteo de 'ID' que han tenido cambios en 'Job Family'
print("Cantidad de 'ID' que han tenido cambios en 'Job Family':", count_changes_in_job_family)

#¿Puede ser que tome todos los job Family de 2022 como nuevos o como cambio? Si no, no se explicarme una cifra tan alta: Luis respondió esto: Las familias es un concepto que antes era local(por país)
#y se ha ido armonizando a un catalogo común, que se compone de 54 familias.

# Rellena los campos vacíos en la columna 'overall_manager_rating' con el valor 2
df_completo_01['overall_manager_rating'].fillna(2, inplace=True)
# Rellena los campos vacíos en la columna 'overall_manager_label' con el valor 'Achieved'
df_completo_01['overall_manager_label'].fillna('Achieved', inplace=True)

# Rellena los campos con valor 0 en la columna 'overall_manager_label' con el valor 'Achieved'
df_completo_01['overall_manager_label'] = df_completo_01['overall_manager_label'].map(lambda x: 'Achieved' if x == 0 else x)
# Cantidad de valores únicos en la columna 'overall_manager_label'
cantidad_valores_unicos = df_completo_01['overall_manager_label'].nunique()

# Lista de valores únicos en la columna 'overall_manager_label'
valores_unicos = df_completo_01['overall_manager_label'].unique()

print("Cantidad de valores únicos en la columna 'overall_manager_label':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'overall_manager_label':", valores_unicos)

# Correlaciones entre todas las columnas
correlacion_df_completo = df_completo_01.corr()
print(correlacion_df_completo)

#Correlación entre columnas de intere#Correlación entre columnas de interes
columnas_para_correlacion = ['overall_manager_label', 'overall_manager_rating', 'spain_of_control', 'Valoracion 360 Global', 'Valoracion 360 Autovaloracion', 'Valoracion 360 Colaboradores']
df_columnas_correlacion = df_completo_01[columnas_para_correlacion]

#Correlación entre la columna de interes con el resto del nuevo DataFrame
column_interes = 'overall_manager_label'
correlations_with_column = df_columnas_correlacion.corr(method='pearson')
print(correlations_with_column)

#no vemos que esto aporte nada
#column_interes1 = 'overall_manager_rating'
#correlations_with_column1 = df_columnas_correlacion.corr(method='pearson')
#print(correlations_with_column1)

#mapa de correlación general
plt.figure(figsize=(50, 50))
sns.heatmap(correlacion_df_completo, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Mapa de Correlaciones')
plt.show()

#mapa correlación overall_manager_lebel
plt.figure(figsize=(10, 8))
sns.heatmap(correlations_with_column, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Mapa de Correlaciones')
plt.show()

# Cantidad de valores únicos en la columna 'mar_status'
cantidad_valores_unicos = df_completo_01['mar_status'].nunique()

# Lista de valores únicos en la columna 'mar_status'
valores_unicos = df_completo_01['mar_status'].unique()

print("Cantidad de valores únicos en la columna 'mar_status':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'mar_status':", valores_unicos)

# Cantidad de valores únicos en la columna 'field_study'
cantidad_valores_unicos = df_completo_01['field_study'].nunique()

# Lista de valores únicos en la columna 'field_study'
valores_unicos = df_completo_01['field_study'].unique()

print("Cantidad de valores únicos en la columna 'field_study':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'field_study':", valores_unicos)

# Cantidad de valores únicos en la columna 'Management Level'
cantidad_valores_unicos = df_completo_01['Management Level'].nunique()

# Lista de valores únicos en la columna 'Management Level'
valores_unicos = df_completo_01['Management Level'].unique()

print("Cantidad de valores únicos en la columna 'Management Level':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'Management Level':", valores_unicos)

# Cantidad de valores únicos en la columna 'Job Family'
cantidad_valores_unicos = df_completo_01['Job Family'].nunique()

# Lista de valores únicos en la columna 'Job Family'
valores_unicos = df_completo_01['Job Family'].unique()

print("Cantidad de valores únicos en la columna 'Job Family':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'Job Family':", valores_unicos)

"""Para unificar el valor de "Job Family" de cada "ID" al valor de "Job Family" cuando "year_performance" = 2022 según la decisión tomada de tener sólo en cuenta el valor actual vigente del "Job Family":

1. Primero, vamos a filtrar el DataFrame para obtener solo las filas que tienen "year_performance" igual a 2022.
2. Luego, vamos a crear un diccionario que mapee cada "ID" al valor de "Job Family" correspondiente para el año 2022.
3. Finalmente, actualizaremos el DataFrame utilizando el diccionario creado para unificar el valor de "Job Family" de cada "ID".

Después de ejecutar este código, el DataFrame `df_completo_01` tendrá la columna "Job Family" actualizada con los valores unificados de 2022 para cada "ID".
"""

df_completo_01.shape

df_completo_01.columns

# Paso 1: Filtrar las filas con "year_performance" igual a 2022
df_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]

# Paso 2: Crear un diccionario que mapee cada "ID" al valor de "Job Family" de 2022
id_to_job_family_2022 = df_2022.set_index('ID')['Job Family'].to_dict()

# Paso 3: Actualizar el DataFrame original con los valores unificados de "Job Family"
df_completo_01['Job Family'] = df_completo_01['ID'].map(id_to_job_family_2022)

# Imprimir los valores únicos después de la actualización
cantidad_valores_unicos = df_completo_01['Job Family'].nunique()
valores_unicos = df_completo_01['Job Family'].unique()

print("Cantidad de valores únicos en la columna 'Job Family':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'Job Family':", valores_unicos)

#Filtrar las entradas con "ID" igual a "AA001" para comprobar que el Job Family es igual en los tres años
filtro = df_completo_01["ID"] == "AA001"
entradas_con_id_concreto = df_completo_01[filtro]
print(entradas_con_id_concreto)

"""La función revisa cuál es el valor de "Job Family" y lo incluye en una nueva columna que indica el "Job Family Group" correspondiente"""

#Definir la función para obtener el grupo de "Job Family"
def get_job_family_group(job_family):
    if job_family in ['Distribution Management', 'Customer S&S: F2F', 'Customer S&S: Remote', 'Customer S&S: Specialized products', 'Commercial & Business Banker', 'Banker', 'Sales', 'Transaction Banking', 'Debt Finance', 'M&A', 'Investment & Asset Management']:
        return "Customer Facing"
    elif job_family in ['Research and Business Intelligence', 'Product & Service Value Proposition Management', 'Business Development & Partnerships', 'Trading', 'Middle Office']:
        return "Non-customer Facing"
    elif job_family in ['Audit', 'Compliance', 'Recovery & Collections', 'EWRM', 'Credit Risk', 'Market & Structural Risk', 'Non-financial Risk']:
        return "Internal Control"
    elif job_family in ['Accounting', 'Analysis & Control', 'Financial Management', 'Capital']:
        return "Management Control"
    elif job_family in ['Software Engineering', 'Systems Infrastructure & Communication Platforms']:
        return "IT Delivery"
    elif job_family in ['Product & IT Project Management']:
        return "Product & IT Project Management"
    elif job_family in ['Solution Application, Infrastructure & Data Architecture', 'Technology Risk & Cybersecurity']:
        return "IT Enterprise"
    elif job_family in ['Service & Delivery Management', 'End User Technologies']:
        return "Service Support"
    elif job_family in ['Data Analytics & Models', 'Data Management & Governance']:
        return "Data"
    elif job_family in ['Admin & Support', 'Properties & Gral. Services', 'Procurement & Third Party Cost Mgmt.']:
        return "General Services"
    elif job_family in ['HR Specialist', 'HR Management']:
        return "Human Resources"
    elif job_family in ['Operations']:
        return "Operations"
    elif job_family in ['Investor Relations', 'Supervisory & Public Stakeholder Management', 'Communication', 'Marketing']:
        return "Skateholder Management & Marketing"
    elif job_family in ['Tax', 'Legal', 'Governance']:
        return "Counsel & Governance"
    elif job_family in ['Top Management', 'Strategy & Corporate Development', 'Project', 'Early Careers']:
        return "Other Staff & Support"
    else:
        return None

#Agregar una nueva columna "Job Family Group" al DataFrame
df_completo_01["Job Family Group"] = df_completo_01["Job Family"].apply(get_job_family_group)

"""La función revisa cuál es el valor de "Job Family Group" y lo incluye en una columna nueva que indica el "Job Family Category" correspondiente"""

#Definir la función para obtener "Job Family Category"
def get_job_family_category(job_family_group):
    if job_family_group in ['Customer Facing', 'Non-customer Facing']:
        return "Business"
    elif job_family_group in ['Internal Control', 'Management Control']:
        return "Control & Oversight"
    elif job_family_group in ['T Delivery', 'Product & IT Project Management', 'IT Enterprise', 'Service Support', 'Data', 'General Services', 'Human Resources', 'Operations', 'Skateholder Management & Marketing', 'Counsel & Governance', 'Other Staff & Support']:
        return "Staff & Support"
    else:
        return None

#Agregar una nueva columna "Job Family Group" al DataFrame
df_completo_01["Job Family Category"] = df_completo_01["Job Family Group"].apply(get_job_family_category)

df_completo_01.shape

"""##Convertimos en categórica la variable "Gender"
"""

#Codificación one-hot para 'Gender'
df_completo_01_encoded = pd.get_dummies(df_completo_01, columns=["Gender"],prefix="")

df_completo_01_encoded.shape

df_completo_01_encoded.head(3)

"""##Para convertir las variables categóricas "Job Family Group" y "Job Family Category" en variables cuantitativas, puedes usar la técnica de codificación one-hot o de etiquetado numérico:

1.   Codificación one-hot: La codificación one-hot crea una columna binaria para cada categoría en la variable original. Cada columna binaria representa la presencia o ausencia de la categoría en la observación. Se puede hacer esto usando la función get_dummies de pandas para la columna "Job Family Group" y la columna "Job Family Category":
"""

#Codificación one-hot para 'Job Family Group'
df_completo_01_ohe = pd.get_dummies(df_completo_01_encoded, columns=["Job Family Group"], prefix="JFG")

df_completo_01_ohe.shape

# Codificación one-hot para 'Job Family Category'
df_completo_01_ohe = pd.get_dummies(df_completo_01_ohe, columns=["Job Family Category"], prefix="JFC")

df_completo_01_ohe.shape

df_completo_01_ohe.head(3)

"""

2.   Etiquetado numérico: El etiquetado numérico asigna un número entero a cada categoría en función de su posición en la lista de categorías. Se puede usar el método factorize de pandas para realizar el etiquetado numérico:

"""

df_completo_01_et_num=df_completo_01.copy()

#Etiquetado numérico para 'Job Family Group'
df_completo_01_et_num["JFG_numerical"] = pd.factorize(df_completo_01_encoded["Job Family Group"])[0]

df_completo_01_et_num.shape

# Etiquetado numérico para 'Job Family Category'
df_completo_01_et_num["JFC_numerical"] = pd.factorize(df_completo_01_et_num["Job Family Category"])[0]

df_completo_01_et_num.shape

df_completo_01_et_num.head(3)

"""ESTUDIO DE  LA RELACIÓN DE LAS JOB FAMILY CON LEVESHNTEIN"""

#Dado que en el caso de las job family ha habido cambios en la denominación de las mismas en el año 2022 con respecto
#a las existentes en los años 2020 y 2021, vamos a buscar la relación que puede existir entre estas nuevas familias
#y las existentes con anterioridad. Para ello vamos a utilizar la distancia de Levenshtein y calcular la posible similitud
#entre las diferentes familias.
#En primer lugar vamos a transformar algunos de los datos que aparecen con acrónimos en su correspondiente traducción.

df_2022.loc[df_2022['Job Family'] == 'EWRM', 'Job Family'] = 'Enterprise Wide Risk Management'
df_2022.loc[df_2022['Job Family'] == 'CIB', 'Job Family'] = 'Corporate and Investment bank'
df_2022.loc[df_2022['Job Family'] == 'M&A', 'Job Family'] = 'Merger and Acquistions'

df_jobfamily_22=df_2022.filter(['Job Family'])
df_jobfamily_22

"""Renombramos la columna de Job Family


"""

df_jobfamily_22.columns = ['JobFamily_22' if x=='Job Family' else x for x in df_jobfamily_22.columns]
df_jobfamily_22

df_2021.loc[df_2021['Job Family'] == 'EWRM', 'Job Family'] = 'Enterprise Wide Risk Management'
df_2021.loc[df_2021['Job Family'] == 'CIB', 'Job Family'] = 'Corporate and Investment bank'
df_2021.loc[df_2021['Job Family'] == 'M&A', 'Job Family'] = 'Merger and Acquistions'

df_jobfamily_21=df_2021.filter(['Job Family'])
df_jobfamily_21

"""Eliminamos (NEW) en el dataset de 2021 ya que no aporta valor alguno al análisis:"""

df_jobfamily_21 ['Job Family']=df_jobfamily_21 ['Job Family'].str.strip('(NEW)')
df_jobfamily_21

df_jobfamily_22['JobFamily_22'] =df_jobfamily_22['JobFamily_22'].astype(str)

"""Renombramos la columna de Job Family


"""

df_jobfamily_21.columns = ['JobFamily_21' if x=='Job Family' else x for x in df_jobfamily_21.columns]
df_jobfamily_21

df_jobfamily_21['JobFamily_21'] =df_jobfamily_21['JobFamily_21'].astype(str)

"""Ahora con las dos columnas de los años 2021 y 2022 preparadas con los datos limpios, realizamos el cálculo de la distancia de Levenshtein para intentar relacionar las familias de 2021 con las nuevas creadas en 2022"""

!pip install Levenshtein
import Levenshtein
from itertools import product

levenshtein_distances = []

for index, row1 in df_jobfamily_21.iterrows():
    for index, row2 in df_jobfamily_22.iterrows():
        distance = Levenshtein.distance(row1['JobFamily_21'], row2['JobFamily_22'])
        levenshtein_distances.append(distance)

distance_df = pd.DataFrame({'Levenshtein_Distance': levenshtein_distances})
distance_df

"""Dada la disparidad entre las nuevas familias del 2022 y las anteriores,no nos es posible establecer una relación entre ellas; hemos hecho este mismo análisis "a mano" en excel con idénticos resultados. Por ello establecemos las familias del 2022 como las definitivas y las trasladamos a los años anteriores, a riesgo de perder cierta trazabilidad pero que asumimos como poco importante a la hora de realizar análisis posteriores.

ANÁLISIS DE CORRELACIÓN DE LAS VARIABLES DEL DATAFRAME CON ONE HOT ENCODING

Una vez transformado las variables categóricas en variables numéricas mediante one hot encoding en el dataframe, ralizaremos un análisis de correlación teniendo en cuenta los valores por encima de 0.7.
"""

df = df_completo_01_et_num
correlation_matrix = df.corr()

plt.figure(figsize=(35, 35))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=0.7)
plt.title('Correlation Heatmap (encoded) with Values > 0.7')
plt.show()

df_completo_01_ohe.info()

columns_to_normalize=['n_children','spain_of_control','pct_women','pct_below40','pct_above60','avg_age_sub','avg_ten_sub','pct_corp_seg','pct_STEM',
         'pct_mngt_lvl','what_performance_rating_h','what_performance_rating_f','how_performance_rating',
         'risk_performance_rating','overall_manager_rating','emp_what_perf_rating','emp_how_perf_rating','emp_rsk_perf_rating','overall_employee_rating','Valoracion 360 Global',
         'Think Customer – Pienso en el cliente - Global','Embrace Change – Impulso el cambio - Global','Act Now – Actúo con rapidez - Global','Move Together – Trabajo en equipo - Global',
         'Speak Up – Hablo abiertamente - Global','Valoracion 360 Autovaloracion','Think Customer – Pienso en el cliente - Autovaloracion','Embrace Change – Impulso el cambio - Autovaloracion',
         'Act Now – Actúo con rapidez - Autovaloracion','Move Together – Trabajo en equipo - Autovaloracion','Speak Up – Hablo abiertamente - Autovaloracion','Valoracion 360 Manager',
         'Think Customer – Pienso en el cliente - Manager','Embrace Change – Impulso el cambio - Manager','Act Now – Actúo con rapidez - Manager','Move Together – Trabajo en equipo - Manager',
         'Speak Up – Hablo abiertamente - Manager','Valoracion 360 Colaboradores','Think Customer – Pienso en el cliente - Colaboradores','Embrace Change – Impulso el cambio - Colaboradores',
         'Act Now – Actúo con rapidez - Colaboradores','Move Together – Trabajo en equipo - Colaboradores','Speak Up – Hablo abiertamente - Colaboradores']

data_to_normalize = df_completo_01_ohe[columns_to_normalize]

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_df = scaler.fit_transform(data_to_normalize)

scaled_df.shape

print(columns_to_normalize)

data_to_normalize.shape

df_completo_01_ohe.shape

df_completo_01_ohe_scaled = pd.DataFrame(scaled_df, columns=columns_to_normalize)

df_completo_01_ohe.head(3)

df_completo_01_ohe_scaled.head(10)

# Veamos la asociación entre:
plt.scatter(df_completo_01_ohe['pct_women'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('pct_women')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['pct_below40'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('pct_below40')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['avg_age_sub'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('avg_age_sub')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['avg_ten_sub'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('avg_ten_sub')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['pct_STEM'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('pct_STEM')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['pct_mngt_lvl'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('pct_mngt_lvl')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['risk_performance_rating'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('risk_performance_rating')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['overall_employee_rating'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('overall_employee_rating')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['Valoracion 360 Global'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('Valoracion 360 Global')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['Think Customer – Pienso en el cliente - Manager'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('Think Customer – Pienso en el cliente - Manager')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['Embrace Change – Impulso el cambio - Manager'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('Embrace Change – Impulso el cambio - Manager')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['Act Now – Actúo con rapidez - Manager'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('Act Now – Actúo con rapidez - Manager')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['Move Together – Trabajo en equipo - Manager'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('Move Together – Trabajo en equipo - Manager')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['Speak Up – Hablo abiertamente - Manager'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('Speak Up – Hablo abiertamente - Manager')
plt.ylabel('overall_manager_rating')
plt.show()

plt.scatter(df_completo_01_ohe['num_position_levels'],df_completo_01_ohe['overall_manager_rating'])
plt.xlabel('num_position_levels')
plt.ylabel('overall_manager_rating')
plt.show()

print(df_completo_01_ohe.describe())

print(df_completo_01_ohe['Country of Origin'].value_counts())

# Utilizando Seaborn para un gráfico de densidad
sns.kdeplot(df_completo_01_ohe['overall_manager_rating'], fill=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['num_position_levels'], fill=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['Valoracion 360 Global'], fill=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['Age'], shade=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['risk_performance_rating'], fill=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['overall_employee_rating'], shade=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['Think Customer – Pienso en el cliente - Manager'], shade=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['Embrace Change – Impulso el cambio - Manager'], shade=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['Act Now – Actúo con rapidez - Manager'], shade=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['Move Together – Trabajo en equipo - Manager'], shade=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

sns.kdeplot(df_completo_01_ohe['Speak Up – Hablo abiertamente - Manager'], shade=True)
plt.xlabel('Valor')
plt.ylabel('Densidad')
plt.title('Gráfico de Densidad de Variable Numérica')
plt.show()

"""#CLUSTERIFICACIÓN:
K means,
DB scan,
K prototypes
"""

!pip install kmodes
!pip install kneed
from kmodes.kprototypes import KPrototypes
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from kneed import KneeLocator
from sklearn.datasets import make_blobs
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

"""**Kmeans**

Observamos que en el dataframe en el que se ha aplicado el one hot encoding para transformar las variables categóricas en variables numéricas existen valores NaN, valores que el algoritmo no puede procesar. Por ello, vamos a susituir los valores NaN por 0:
"""

df_completo_01_ohe_scaled.fillna(0, inplace=True)

kmeans_kwargs = {
   ...:     "init": "random",
   ...:     "n_init": 10,
   ...:     "max_iter": 300,
   ...:     "random_state": 44,
   ...: }
   ...:
   ...: # A list holds the SSE values for each k
   ...: sse = []
   ...: for k in range(1, 15):
   ...:     kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
   ...:     kmeans.fit(df_completo_01_ohe_scaled)
   ...:     sse.append(kmeans.inertia_)

"""Cuando se grafica la SSE en función del número de clústeres, se observa que la SSE continúa disminuyendo a medida que aumenta k. A medida que se agregan más centroides, la distancia desde cada punto hasta su centroide más cercano disminuirá.

Hay un punto dulce donde la curva SSE comienza a doblarse conocida como el punto del codo. Se asume que el valor x de este punto es una compensación razonable entre el error y el número de clústeres.
"""

plt.style.use("fivethirtyeight")
plt.plot(range(1, 15), sse)
plt.xticks(range(1, 15))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

"""Podemos usar kneed para identificar el codo mediante programación:"""

kl = KneeLocator(range(1, 15), sse, curve="convex", direction="decreasing")

kl.elbow

"""Como podemos observar tanto en la gráfica como con el codigo kneed, el número de cluster óptimo es 4"""

kmeans = KMeans(
   ...:     init="random",
   ...:     n_clusters=4,
   ...:     n_init=10,
   ...:     max_iter=300,
   ...:     random_state=44
   ...: )

kmeans.fit(df_completo_01_ohe_scaled)

kmeans.inertia_ ##valor de SSE más bajo##

kmeans.cluster_centers_ ##localizaciones finales de los centroides##

kmeans.n_iter_ ##número de iteraciones requeridas para converger##

kmeans.labels_[:10] ##10 primeras etiquetas predichas##

preprocessor = Pipeline([("scaler", MinMaxScaler()),("pca", PCA(n_components=43, random_state=43))])

clusterer = Pipeline([("kmeans",KMeans(init="k-means++",n_init=50,max_iter=500,random_state=43))])

pipe = Pipeline([("preprocessor", preprocessor),("clusterer", clusterer)])

pipe.fit(df_completo_01_ohe_scaled)

cluster_assignments = kmeans.labels_

df_completo_01_ohe_scaled['cluster'] = cluster_assignments

cluster_centers = kmeans.cluster_centers_

cluster_centers_df = pd.DataFrame(cluster_centers, columns=df_completo_01_ohe_scaled.columns)
print(cluster_centers_df)

cluster_sizes = cluster_centers_df['cluster'].value_counts()
print(cluster_sizes)

cluster_stats = df_completo_01_ohe_scaled.groupby('cluster').mean()
print(cluster_stats)

"""Evaluamos el rendimiento calculando el coeficiente de silueta (silhouete). Un coeficiente de silueta de 0 indica que los grupos se superponen significativamente entre sí,
y un coeficiente de silueta de 1 indica que los grupos están bien separados.
"""

preprocessed_data = pipe["preprocessor"].transform(df_completo_01_ohe_scaled)

predicted_labels = pipe["clusterer"]["kmeans"].labels_

silhouette_score(preprocessed_data, predicted_labels)

"""El coeficiente de silhouete nos indica que existe superposición entre los 4 clúster identificados en el Kmeans.

**DBScan (Density-Based Spatial Clustering of Applications with Noise)**
"""

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_completo_01_ohe_scaled)

dbscan = DBSCAN(eps=0.5, min_samples=7)
clusters = dbscan.fit_predict(df_completo_01_ohe_scaled)
df_completo_01_ohe_scaled['cluster'] = clusters

from sklearn.decomposition import PCA

pca = PCA(n_components=43)
pca_result = pca.fit_transform(df_completo_01_ohe_scaled.drop('cluster', axis=1))

plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='viridis')
plt.title('DBSCAN Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

epsilon = 0.5 # The radius within which to search for nearby points
min_samples = 7  # The minimum number of points required to form a dense region
dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)
cluster_labels = dbscan.fit_predict(df_scaled)

unique_clusters = set(cluster_labels)
num_clusters = len(unique_clusters)
num_noise_points = sum(1 for label in cluster_labels if label == -1)

cluster_statistics = []
for cluster in unique_clusters:
    num_points_in_cluster = (cluster_labels == cluster).sum()
    cluster_statistics.append({
        'Cluster': cluster,
        'Number of Points': num_points_in_cluster,
    })

cluster_stats_df = pd.DataFrame(cluster_statistics)

print("Number of clusters:", num_clusters)
print("Number of noise points:", num_noise_points)
print("\nCluster statistics:")
print(cluster_stats_df)

"""Observamos que tenemos 6476 puntos de ruido en nuestro análisis de DB SCan, valor que no varía significativamente al aplicar diferentes valores a epsilon y a min_samples (siempre están por encima de 6000), estos puntos de ruido son valores que no cumplen los requisitos que hemos marcado, es decir, están fuera de cualquier clúster identificado. Esto nos lleva a interpretar que nuestros datos no están formando clústeres.

**K prototypes**

Identificamos las columnas con valores categóricas
"""

categorical_columns = df_completo_01_ohe_scaled.select_dtypes(include=['object', 'category']).columns
categorical_columns

"""No se identifican variables categóricas, por lo que no se puede aplicar el algoritmo de K prototypes

# **Cálculo de regresión**

**Análisis de regresión lineal multivariante**

Dado que nuestros datos no se adaptan a agrupaciones por clúster, vamos a calcular la regresión de los mismos. Empezaremos con un análisis multivariante de regresión lineal
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

##Definimos la variable dependiente como la valoración global y las variables independientes como el resto de variables que aparecen en nuestro data frame##

print(nombres_columnas)

X = df_completo_01_ohe_scaled[['n_children', 'spain_of_control', 'pct_women', 'pct_below40',
       'pct_above60', 'avg_age_sub', 'avg_ten_sub', 'pct_corp_seg', 'pct_STEM',
       'pct_mngt_lvl', 'what_performance_rating_h',
       'what_performance_rating_f', 'how_performance_rating',
       'risk_performance_rating', 'overall_manager_rating',
       'emp_what_perf_rating', 'emp_how_perf_rating', 'emp_rsk_perf_rating',
       'overall_employee_rating',
       'Think Customer – Pienso en el cliente - Global',
       'Embrace Change – Impulso el cambio - Global',
       'Act Now – Actúo con rapidez - Global',
       'Move Together – Trabajo en equipo - Global',
       'Speak Up – Hablo abiertamente - Global',
       'Valoracion 360 Autovaloracion',
       'Think Customer – Pienso en el cliente - Autovaloracion',
       'Embrace Change – Impulso el cambio - Autovaloracion',
       'Act Now – Actúo con rapidez - Autovaloracion',
       'Move Together – Trabajo en equipo - Autovaloracion',
       'Speak Up – Hablo abiertamente - Autovaloracion',
       'Valoracion 360 Manager',
       'Think Customer – Pienso en el cliente - Manager',
       'Embrace Change – Impulso el cambio - Manager',
       'Act Now – Actúo con rapidez - Manager',
       'Move Together – Trabajo en equipo - Manager',
       'Speak Up – Hablo abiertamente - Manager',
       'Valoracion 360 Colaboradores',
       'Think Customer – Pienso en el cliente - Colaboradores',
       'Embrace Change – Impulso el cambio - Colaboradores',
       'Act Now – Actúo con rapidez - Colaboradores',
       'Move Together – Trabajo en equipo - Colaboradores',
       'Speak Up – Hablo abiertamente - Colaboradores']]
y = df_completo_01_ohe_scaled['Valoracion 360 Global']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

import statsmodels.api as sm

X = sm.add_constant(X)

modelo = sm.OLS(y, X).fit()

print(modelo.summary())

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

##Analizamos los valores estadísticos obtenidos en nuestro análisis de regresión:##

"""MSE: El valor obtenido de MSE (Mean Squared Error) de 0.0003909335480122403 nos indica que el modelo evaluado evaluando tiene un bajo nivel de error cuadrático medio en la predicción de los valores observados en los datos. El MSE es una medida de la calidad de ajuste de un modelo de regresión, y se utiliza para evaluar como se ajustan las predicciones del modelo con los valores reales.

En este caso, un valor de MSE tan bajo indica que las predicciones del modelo están muy cerca de los valores reales. Cuanto más cercano a cero sea el valor de MSE, mejor será el ajuste del modelo a los datos.

Un MSE de 0.0003909335480122403 sugiere que el modelo de regresión es bastante preciso en la predicción de los valores del conjunto de datos.

Durbin-Watson: es una prueba utilizada en análisis de regresión para evaluar la autocorrelación de los residuos, es decir, si hay patrones sistemáticos en los errores de predicción que podrían violar uno de los supuestos fundamentales de la regresión lineal múltiple: la independencia de los errores.

El valor del estadístico Durbin-Watson puede variar entre 0 y 4. Un valor de 2 indica que no hay autocorrelación serial en los residuos, lo que es deseable en un análisis de regresión. Valores cercanos a 0 sugieren autocorrelación positiva (los residuos tienden a estar correlacionados positivamente entre sí), mientras que valores cercanos a 4 sugieren autocorrelación negativa (los residuos tienden a estar correlacionados negativamente entre sí).

Un valor de Durbin-Watson de 1.882 se encuentra cerca de 2, lo que sugiere que hay muy poca evidencia de autocorrelación serial en los residuos del análisis de regresión multivariante. Esto es una buena noticia, ya que indica que los residuos se comportan de manera independiente, lo que es importante para la validez de las inferencias y predicciones realizadas a partir del modelo de regresión.

Kurtosis: una medida estadística que describe la forma de la distribución de datos en un conjunto de datos. Más específicamente, la kurtosis se relaciona con la concentración de datos en la cola de la distribución en comparación con la distribución normal (conocida como la distribución de campana o forma de campana). Una kurtosis positiva indica que la distribución tiene colas más pesadas (más valores extremos) en comparación con la distribución normal, mientras que una kurtosis negativa indica colas más ligeras (menos valores extremos) que la distribución normal.

En el contexto de un análisis multivariante de regresión, una kurtosis de 9.895 sugiere que la distribución de los errores (residuos) del modelo de regresión tiene colas muy pesadas, lo que significa que hay valores extremos o atípicos en los datos de residuos que están alejados de la media en comparación con lo que se esperaría bajo una distribución normal. Estos valores extremos pueden indicar que el modelo de regresión no se ajusta bien a los datos en algunas áreas o que hay otros factores no tenidos en cuenta que afectan a la distribución de los residuos.

En resumen, una kurtosis alta en un análisis multivariante de regresión puede indicar problemas con la suposición de normalidad de los residuos, lo que puede afectar la validez de las pruebas estadísticas y las inferencias realizadas sobre el modelo de regresión. Sería importante investigar más a fondo los datos y considerar si es necesario tomar medidas para abordar los valores atípicos o ajustar el modelo de regresión.

Skew: asimetría en la distribución de los datos en un análisis estadístico. Un valor de skew de -1.047 indica que la distribución de los datos está sesgada hacia la izquierda (o hacia la cola izquierda) de la distribución. Esto significa que la mayoría de los valores se encuentran en el extremo derecho de la distribución, y hay una cola larga en el lado izquierdo con valores más bajos.

En el contexto de un análisis multivariante de regresión, este valor de skew podría referirse a la asimetría en la distribución de los residuos del modelo de regresión. Los residuos son las diferencias entre los valores observados y los valores predichos por el modelo de regresión. Un skew negativo en los residuos indica que los residuos tienden a ser más pequeños y negativos, lo que significa que el modelo tiende a sobreestimar los valores observados en lugar de subestimarlos.

Prueba de Goldfeld-Quandt: La prueba de Goldfeld-Quandt es una prueba estadística utilizada para verificar la heterocedasticidad en un modelo de regresión. Se aplica comúnmente en modelos de regresión lineal simple, pero no es tan común en análisis multivariados de regresión. Sin embargo, puedes realizar una versión adaptada de la prueba de Goldfeld-Quandt en un análisis multivariante de regresión si tienes razones para sospechar que la heterocedasticidad es un problema en tu modelo.
"""

import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_goldfeldquandt
test_statistic, p_value, _ = het_goldfeldquandt(modelo.resid, modelo.model.exog)
print(f"Estadístico de prueba de Goldfeld-Quandt: {test_statistic}")
print(f"Valor p: {p_value}")

# Interpretación de los resultados
if p_value < 0.05:
    print("Hay evidencia de heterocedasticidad.")
else:
    print("No hay evidencia suficiente para concluir que existe heterocedasticidad.")

"""Prueba de White: una prueba estadística utilizada para evaluar la heterocedasticidad en un modelo de regresión."""

df_completo_01_ohe_scaled = sm.add_constant(df_completo_01_ohe_scaled)
model = sm.OLS(y, X)
results = model.fit()

white_test = sm.stats.diagnostic.het_white(results.resid, exog=results.model.exog)

# Imprimimos el resultado de la prueba.
print("Estadística de la prueba de White:", white_test[0])
print("P-valor de la prueba de White:", white_test[1])
print("Valor crítico 1 de la prueba de White:", white_test[2])
print("Valor crítico 2 de la prueba de White:", white_test[3])

"""1. **Estadística de la prueba de White (White Test Statistic)**: La estadística de la prueba es 4559.729732156484. Cuanto mayor sea este valor, mayor será la evidencia en contra de la hipótesis nula de que no hay heteroscedasticidad en el modelo.

2. **P-valor de la prueba de White (White Test P-Value)**: El p-valor es la probabilidad de obtener una estadística de prueba igual o más extrema que la observada si la hipótesis nula (en este caso, la ausencia de heteroscedasticidad) fuera verdadera. Un p-valor bajo, en este caso 0.0, generalmente sugiere que hay evidencia significativa en contra de la hipótesis nula. En otras palabras, en base a este p-valor, podríamos rechazar la hipótesis nula y concluir que existe heteroscedasticidad en el modelo.

3. **Valor crítico 1 de la prueba de White**: Este valor crítico se utiliza para comparar con la estadística de la prueba. Si la estadística de la prueba es mayor que este valor crítico, se considera evidencia en contra de la hipótesis nula. En este caso, el valor crítico 1 es 12.842691445332745.

4. **Valor crítico 2 de la prueba de White**: Este es otro valor crítico que se utiliza para la comparación. Si la estadística de la prueba es mayor que este valor crítico, también se considera evidencia en contra de la hipótesis nula. En este caso, el valor crítico 2 es 0.0.

Dado que la estadística de la prueba es mucho mayor que ambos valores críticos y el p-valor es extremadamente bajo (cercano a cero), esto sugiere fuertemente que hay evidencia de heteroscedasticidad en el modelo, lo que significa que la varianza de los errores no es constante a lo largo de los datos y que el modelo de regresión lineal podría no ser apropiado sin correcciones.

**Análisis de regresión polinómica multivariante**
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

degree = 2
poly = PolynomialFeatures(degree=degree)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

model = LinearRegression()
model.fit(X_train_poly, y_train)

y_pred = model.predict(X_test_poly)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Error cuadrático medio (MSE): {mse}")
print(f"Coeficiente de determinación (R^2): {r2}")

coeficientes = model.coef_
intercepto = model.intercept_

# Imprimir los coeficientes y el intercepto
print("Coeficientes:", coeficientes)
print("Intercepto:", intercepto)

degree = 3
poly = PolynomialFeatures(degree=degree)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

model = LinearRegression()
model.fit(X_train_poly, y_train)

y_pred = model.predict(X_test_poly)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Error cuadrático medio (MSE): {mse}")
print(f"Coeficiente de determinación (R^2): {r2}")

"""Grado 3: tiene un bajo error cuadrático medio y explica una gran parte de la variabilidad en la variable dependiente, lo que sugiere que es un modelo sólido y que se ajusta bien a los datos."""



"""**Random forest de regresión**"""

df_completo_01_ohe_scaled. info

##Dividimos nuestro dataset en dos dataset: el primero tendrá un 90% de los datos y es el que usaremos para realizar el random forest y el segundo tendrá el 10% de los datos y lo##
##utilizaremos como datos de validación##

total_filas = len(df_completo_01_ohe_scaled)
filas_10porciento = int(6616 * 0.10)
filas_90porciento = total_filas - filas_10porciento
df_10porciento = df_completo_01_ohe_scaled.sample(n=filas_10porciento, random_state=42)
df_90porciento = df_completo_01_ohe_scaled.drop(df_10porciento.index)

r2 = r2_score(y_test, y_pred)
print(f'R-squared: {r2}')

# Importa las bibliotecas necesarias
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Supongamos que tu objetivo es predecir la columna 'target' (tu variable de respuesta)
# y que las características se encuentran en las columnas restantes de tu DataFrame.

# Divide tus datos en conjuntos de entrenamiento y prueba
X = df_90porciento.drop('Valoracion 360 Global', axis=1)
y = df_90porciento['Valoracion 360 Global']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Crea el modelo de Random Forest de regresión
rf_regressor = RandomForestRegressor(n_estimators=50,max_depth=10,random_state=42)

# Entrena el modelo en los datos de entrenamiento
rf_regressor.fit(X_train, y_train)

# Realiza predicciones en el conjunto de prueba
y_pred = rf_regressor.predict(X_test)

# Evalúa el rendimiento del modelo (por ejemplo, usando el error cuadrático medio)
mse = mean_squared_error(y_test, y_pred)
print(f"Error Cuadrático Medio (MSE): {mse}")
print(f'R-squared (R^2): {r2}')

# También puedes analizar la importancia de las características si lo deseas
feature_importances = rf_regressor.feature_importances_
print("Importancia de las características:")
for feature, importance in zip(X.columns, feature_importances):
    print(f"{feature}: {importance}")

from sklearn.model_selection import cross_val_score

X = df_10porciento.drop('Valoracion 360 Global', axis=1)
y = df_10porciento['Valoracion 360 Global']

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

#Utilizamos una cv de 5#
scores = cross_val_score(rf_regressor, X, y, cv=5, scoring='neg_mean_squared_error')

# Los resultados de cross_val_score son negativos, así que los convertiremos en positivos#
rmse_scores = np.sqrt(-scores)

# Calculamos el promedio y la desviación estándar del RMSE#
mean_rmse = rmse_scores.mean()
std_rmse = rmse_scores.std()

print(f'RMSE promedio: {mean_rmse}')
print(f'Desviación estándar del RMSE: {std_rmse}')

"""Serializamos ahora nuestro modelo para posteriomente poder productivizarlo. Para ello utilizaremos joblib:"""

import pickle
import joblib

# Save the model to a file
model_filename = "modelo_rfregression.grupoSantander.pkl"
joblib.dump(rf_regressor, model_filename)

from google.colab import files
files.download(model_filename)

"""**PRODUCTIVIZACIÓN DEL MODELO**
Inferimos que el modelo va a ser utilizado de forma manual, por lo que la herramienta elegida es Streamlit.

"""

!pip install streamlit

import streamlit as st
import joblib

model = RandomForestRegressor(n_estimators=50)
X = df_10porciento.drop('Valoracion 360 Global', axis=1)
y = df_10porciento['Valoracion 360 Global']
model.fit(X, y)

##Importamos nuestro modelo##
model = joblib.load('modelo_rfregression.grupoSantander.pkl')

# Set the title and subtitle
st.title("Forecast of candidates for Manager positions")
st.subheader("Select the value in the different options to obtain an assessment of the candidate")

# Create input fields for each feature
feature1 = st.slider("Think Customer - Pienso en el cliente - Global", 1.00, 2.00, 3.00, 4.00)
feature2 = st.slider("Embrace Change - Impulso el cambio - Global", 1.00, 2.00, 3.00, 4.00)
feature3 = st.slider("Act Now - Actúo con rapidez - Global", 1.00, 2.00, 3.00, 4.00)
feature4 = st.slider("Move together - Trabajo en equipo - Global", 1.00, 2.00, 3.00, 4.00)
feature5 = st.slider("Speak up - Hablo abiertamente - Global", 1.00, 2.00, 3.00, 4.00)


# Create a button to perform the prediction
if st.button("Predict"):
    # Prepare the input data as a DataFrame
    input_data = pd.DataFrame({
        "Think Customer": [feature1],
        "Embrace Change": [feature2],
        "Act Now": [feature3],
        "Move together": [feature4],
        "Speak up": [feature5],
        "Spain of Control": [feature6]
    })

    # Make the prediction using the loaded model
    prediction = model.predict(input_data)[0]

    # Display the prediction
    st.write(f"Valoracion 360 Global: {prediction:.2f}")